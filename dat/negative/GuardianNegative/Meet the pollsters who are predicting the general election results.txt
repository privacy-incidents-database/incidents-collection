Meet the pollsters who are predicting the general election results
https://www.theguardian.com/politics/2015/apr/08/meet-the-pollsters-predicting-general-election-results
More numbers will probably be crunched in this year’s UK general election than in all past elections added together.  We can count it, all right, but we still can’t call it.   Related: The poll of pollsters: with a month to go, it could be Miliband by a whisker    The men with the numbers – and they really are almost always men – are at the beating heart of each of the campaigns. Michael Ashcroft exited the Lords, stage right, by recently resigning his peerage, but he has never been more central to Britain’s political processes than he is today, thanks to the entirely unprecedented scores of constituency polls that he has bankrolled. The high-volume, low-cost potential of online data collection has attracted new polling companies, such as Survation, into the market, and enabled the established ones to churn out their numbers on a nightly basis. But when political scientists convened at the London School of Economics to try to agree on which way this great tide of information was flowing, six academic teams suggested Labour would be victorious, and another six concluded that the Tories had the edge. Part of the reason for the uncertainty is, of course, that most polls reveal that the election is close.    The only way to take a view on whether some companies are more likely to slip up is to gen up on the methods ​they use    Another element of the uncertainty, however, is disagreement about which data you ought to trust. Pollsters deal with the standard margin of error of +/- three points, due to random sampling error but also the great unknown, which is how closely, or not, the people who fill in their forms and pick up their phone calls resemble the electorate as a whole. The best-established firms still live in the shadow of 1992, when the near dead-heat confidently predicted by the industry gave way to an eight-point Tory lead. The only way you can take a view on whether another polling disaster is likely, and whether some companies are more likely than others to slip up, is to gen up on the various players and the methods they use to try to make sure it isn’t them. The bullish Australian architect of the Tory election campaign, Lynton Crosby, is a strategist rather than a pollster himself, but his company Crosby Textor has nonetheless reportedly conducted costly constituency polls in as many as 80 seats. Labour hasn’t got the budget for anything comparable – it does surveys across marginal seats as a whole, not individual constituencies. But now that the campaign is under way, James Morris of Greenberg Quinlan Rosner has been spotted heading for Ed Miliband’s suite of rooms at Westminster on a regular basis. And for the Lib Dems, the South African Ryan Coetzee has moved from being Nick Clegg’s special adviser to take the reins at the party’s Cowley Street HQ – where his tasks have included commissioning bespoke Survation polls in the party’s stronger seats, with the more encouraging results briefed to the press. Leaked data from within the campaigns carries an air of intrigue but it should always be consumed with a healthy pinch of salt. Not because there will be anything wrong with the numbers the parties throw their money at, but because ordinarily they won’t be putting the figures in front of your nose unless it is in their interest to do so – often because they’re outperforming expectations in one particular survey; private polls where they fall behind usually won’t see the light of day. So, in forming a judgment on the true political picture, it is much more important to learn to interpret the various published polls. The single most important distinction is between telephone firms such as Ipsos Mori and ICM, and internet firms such as YouGov and Opinium. Also, each polling company applies different weights and adjustments to the data it collects – for example in terms of how past voting behaviour, undecided voters and different levels of certainty to vote are weighted and filtered (for the more nerdy audience, the differences between pollsters are outlined in more detail below). How the different polls are created The players:ICM (polls regularly for the Guardian)Ipsos Mori (Evening Standard)YouGov (Sun and Sunday Times)ComRes (online, Independent on Sunday; phone, for Daily Mail)Opinium (Observer)Populus (Financial Times)Survation (Daily Mirror)TNSLord Ashcroft  Most polls will have a sample of about 1,000 people, which is weighted to be representative of Britain’s adult population by demographics such as age, geography and gender (ONS or National Readership Survey data is usually used for this). However, there are important differences between each company’s methods. First, the regular Ipsos Mori, ICM and Lord Ashcroft surveys are telephone polls. While YouGov, Populus, Opinium, Survation and TNS are internet polls. ComRes does both. Phone polls use randomised samples, while internet polls are to some extent self-selecting – respondents have to sign up to a panel but cannot choose the poll they respond to. Historically, phone polls have tended to be more accurate, but there is no evidence to suggest that this will always be the case. On the contrary, the fact that fewer people use a landline will in time prove challenging to phone polling (even if nowadays a proportion of the sample is interviewed by mobile phone), especially as internet polling is cheaper, so more data can be collected, and panels become increasingly representative of a population as they grow in size. Second, each polling company will apply different weighting and adjustments to the data it collects – and this is where the main differences between thefirms appear. There are several areas that are common to most of the polling companies:  All but Ipsos Mori weight voting intention based on past voting. The majority of companies do this by asking voters to recall who they voted for in 2010. Others, YouGov being the most notable, normally use 2010 voter identification (the party that voters said they most identified with five years ago), although for this election YouGov too will weight by previous voting intention. All but YouGov apply some sort of filter and weighting based on people’s likelihood of voting (however, in proximity to this election even YouGov will take into account likelihood to vote) – respondents who say they aren’t certain to vote are filtered out or weighted down. ICM additionally weights down the responses of those who didn’t vote in 2010, and ComRes applies tighter filters to supporters of smaller parties. “Don’t knows” are either excluded, reallocated based on past voting intention or “squeezed” (asked who they are most likely to vote for). A more recent addition to the mix is whether pollsters prompt for Ukip – meaning if Farage’s party is included as an option alongside the main parties, or is only offered if a respondent answers “others” to the voting intention question. Opinium, ICM and Ipsos Mori don’t prompt for Ukip.   One effect of the different methods used by the pollsters is that each will have varying levels of support for the different parties, resulting, for example, in a particular polling company producing consistently higher Labour scores than other pollsters. These are known as “house effects”. This doesn’t imply that a poll is partisan, but is simply the result of different approaches. At the end of the day, pollsters want to get it right. Reputations are on the line at every election and how each company applies past behaviour to interpret present-day data is a matter of judgment. Fortunately, the fact that there is an election will allow us to see which method works best. Here’s what happened in 2010:        All polls carry a margin of error and levels of confidence. For example, a poll of 1,000 people has a margin of error of about +/- three points and a confidence interval of 95%. In theory, this means 95 times out of 100 the figure in one poll will be within three percentage points of what it would be if you surveyed the entire population. This implies that any change within the margin of error isn’t significant, and that there is always the possibility of a random sampling error. In other words, there is always an element of uncertainty. Because of this, it is always best to look at not just one poll that shows the result we like best, but at the trends across polls.
